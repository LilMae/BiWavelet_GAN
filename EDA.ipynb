{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from scipy import interpolate\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(os.getcwd(),'data')\n",
    "\n",
    "sensor1 = pd.read_csv(os.path.join(data_root,'g1_sensor1.csv'), names=['time', 'normal', 'type1', 'type2', 'type3'])\n",
    "sensor2 = pd.read_csv(os.path.join(data_root,'g1_sensor2.csv'), names=['time', 'normal', 'type1', 'type2', 'type3'])\n",
    "sensor3 = pd.read_csv(os.path.join(data_root,'g1_sensor3.csv'), names=['time', 'normal', 'type1', 'type2', 'type3'])\n",
    "sensor4 = pd.read_csv(os.path.join(data_root,'g1_sensor4.csv'), names=['time', 'normal', 'type1', 'type2', 'type3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(0,140,0.001)\n",
    "y_new1 = []\n",
    "y_new2 = []\n",
    "y_new3 = []\n",
    "y_new4 = []\n",
    "\n",
    "for item in ['normal', 'type1', 'type2', 'type3']:\n",
    "    f_linear1 = interpolate.interp1d(sensor1['time'], sensor1[item], kind ='linear')\n",
    "    y_new1.append(f_linear1(x_new))\n",
    "    f_linear2 = interpolate.interp1d(sensor2['time'], sensor2[item], kind ='linear')\n",
    "    y_new2.append(f_linear1(x_new))\n",
    "    f_linear3 = interpolate.interp1d(sensor3['time'], sensor3[item], kind ='linear')\n",
    "    y_new3.append(f_linear1(x_new))\n",
    "    f_linear4 = interpolate.interp1d(sensor4['time'], sensor4[item], kind ='linear')\n",
    "    y_new4.append(f_linear1(x_new))\n",
    "    \n",
    "sensor1 = pd.DataFrame(np.array(y_new1).T, columns=['normal', 'type1', 'type2', 'type3'])\n",
    "sensor2 = pd.DataFrame(np.array(y_new2).T, columns=['normal', 'type1', 'type2', 'type3'])\n",
    "sensor3 = pd.DataFrame(np.array(y_new3).T, columns=['normal', 'type1', 'type2', 'type3'])\n",
    "sensor4 = pd.DataFrame(np.array(y_new4).T, columns=['normal', 'type1', 'type2', 'type3'])\n",
    "\n",
    "normal_ = pd.concat([sensor1['normal'], sensor2['normal'], sensor3['normal'], sensor4['normal']], axis=1)\n",
    "type1_ = pd.concat([sensor1[\"type1\"], sensor2[\"type1\"], sensor3[\"type1\"], sensor4[\"type1\"]], axis=1)\n",
    "type2_= pd.concat([sensor1[\"type2\"], sensor2[\"type2\"], sensor3[\"type2\"], sensor4[\"type2\"]], axis=1)\n",
    "type3_= pd.concat([sensor1[\"type3\"], sensor2[\"type3\"], sensor3[\"type3\"], sensor4[\"type3\"]], axis=1)\n",
    "\n",
    "normal_.columns = ['s1','s2','s3','s4']\n",
    "type1_.columns  = ['s1','s2','s3','s4']\n",
    "type2_.columns  = ['s1','s2','s3','s4']\n",
    "type3_.columns  = ['s1','s2','s3','s4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 15\n",
    "\n",
    "normal_s1 = np.convolve(normal_['s1'], np.ones(M), 'valid') / M\n",
    "normal_s1 = normal_s1.reshape(len(normal_s1),1)\n",
    "normal_s2 = np.convolve(normal_['s2'], np.ones(M), 'valid') / M\n",
    "normal_s2 = normal_s2.reshape(len(normal_s2),1)\n",
    "normal_s3 = np.convolve(normal_['s3'], np.ones(M), 'valid') / M\n",
    "normal_s3 = normal_s3.reshape(len(normal_s3),1)\n",
    "normal_s4 = np.convolve(normal_['s4'], np.ones(M), 'valid') / M\n",
    "normal_s4 = normal_s4.reshape(len(normal_s4),1)\n",
    "type1_s1 = np.convolve(type1_['s1'], np.ones(M), 'valid') / M\n",
    "type1_s1 = type1_s1.reshape(len(type1_s1),1)\n",
    "type1_s2 = np.convolve(type1_['s2'], np.ones(M), 'valid') / M\n",
    "type1_s2 = type1_s2.reshape(len(type1_s2),1)\n",
    "type1_s3 = np.convolve(type1_['s3'], np.ones(M), 'valid') / M\n",
    "type1_s3 = type1_s3.reshape(len(type1_s3),1)\n",
    "type1_s4 = np.convolve(type1_['s4'], np.ones(M), 'valid') / M\n",
    "type1_s4 = type1_s4.reshape(len(type1_s4),1)\n",
    "type2_s1 = np.convolve(type2_['s1'], np.ones(M), 'valid') / M\n",
    "type2_s1 = type2_s1.reshape(len(type2_s1),1)\n",
    "type2_s2 = np.convolve(type2_['s2'], np.ones(M), 'valid') / M\n",
    "type2_s2 = type2_s2.reshape(len(type2_s2),1)\n",
    "type2_s3 = np.convolve(type2_['s3'], np.ones(M), 'valid') / M\n",
    "type2_s3 = type2_s3.reshape(len(type2_s3),1)\n",
    "type2_s4 = np.convolve(type2_['s4'], np.ones(M), 'valid') / M\n",
    "type2_s4 = type2_s4.reshape(len(type2_s4),1)\n",
    "type3_s1 = np.convolve(type3_['s1'], np.ones(M), 'valid') / M\n",
    "type3_s1 = type3_s1.reshape(len(type3_s1),1)\n",
    "type3_s2 = np.convolve(type3_['s2'], np.ones(M), 'valid') / M\n",
    "type3_s2 = type3_s2.reshape(len(type3_s2),1)\n",
    "type3_s3 = np.convolve(type3_['s3'], np.ones(M), 'valid') / M\n",
    "type3_s3 = type3_s3.reshape(len(type3_s3),1)\n",
    "type3_s4 = np.convolve(type3_['s4'], np.ones(M), 'valid') / M\n",
    "type3_s4 = type3_s4.reshape(len(type3_s4),1)\n",
    "\n",
    "normal_temp = np.concatenate((normal_s1,normal_s2,normal_s3,normal_s4), axis =1)\n",
    "type1_temp = np.concatenate((type1_s1,type1_s2,type1_s3,type1_s4), axis =1)\n",
    "type2_temp = np.concatenate((type2_s1,type2_s2,type2_s3,type2_s4), axis =1)\n",
    "type3_temp = np.concatenate((type3_s1,type3_s2,type3_s3,type3_s4), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(normal_)\n",
    "normal = scaler.transform(normal_temp)\n",
    "type1  = scaler.transform(type1_temp)\n",
    "type2  = scaler.transform(type2_temp)\n",
    "type3  = scaler.transform(type3_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_pd = pd.DataFrame(normal)\n",
    "type1_pd = pd.DataFrame(type1)\n",
    "type2_pd = pd.DataFrame(type2)\n",
    "type3_pd = pd.DataFrame(type3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'normal : {len(normal)}')\n",
    "print(f'type1 : {len(type1)}')\n",
    "print(f'type2 : {len(type2)}')\n",
    "print(f'type3 : {len(type3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = normal[30000:130000][:]\n",
    "type1  = type1[30000:130000][:]\n",
    "type2  = type2[30000:130000][:]\n",
    "type3  = type3[30000:130000][:]\n",
    "\n",
    "normal_train = normal[:][:60000]\n",
    "normal_valid = normal[:][60000:80000] \n",
    "normal_test = normal[:][80000:]\n",
    "\n",
    "type1_train  = type1[:][:60000]\n",
    "type1_valid  = type1[:][60000:80000] \n",
    "type1_test  = type1[:][80000:]\n",
    "\n",
    "type2_train  = type2[:][:60000]\n",
    "type2_valid  = type2[:][60000:80000]\n",
    "type2_test  = type2[:][80000:]\n",
    "\n",
    "type3_train  = type3[:][:60000]\n",
    "type3_valid  = type3[:][60000:80000]\n",
    "type3_test  = type3[:][80000:]\n",
    "\n",
    "train = np.concatenate((normal_train,type1_train,type2_train,type3_train))\n",
    "valid = np.concatenate((normal_valid,type1_valid,type2_valid,type3_valid))\n",
    "test = np.concatenate((normal_test,type1_test,type2_test,type3_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd = pd.DataFrame(train)\n",
    "test_pd = pd.DataFrame(test)\n",
    "valid_pd = pd.DataFrame(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = [0 for x in range(60000)] + [1 for x in range(60000)] + [2 for x in range(60000)] + [3 for x in range(60000)]\n",
    "test_label = [0 for x in range(20000)] + [1 for x in range(20000)] + [2 for x in range(20000)] + [3 for x in range(20000)]\n",
    "valid_label = [0 for x in range(20000)] + [1 for x in range(20000)] + [2 for x in range(20000)] + [3 for x in range(20000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pd['class'] = train_label\n",
    "test_pd['class'] = test_label\n",
    "valid_pd['class'] = valid_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 256\n",
    "\n",
    "dataset = []\n",
    "\n",
    "for idx in range(0, len(data_pd), length):\n",
    "    start_idx = idx\n",
    "    end_idx = idx+length\n",
    "\n",
    "    if len(data_pd) < end_idx:\n",
    "        break\n",
    "    if data_pd['class'][start_idx] != data_pd['class'][end_idx]:\n",
    "        continue\n",
    "    \n",
    "    # 2개쌍으로 데이터 묶기\n",
    "    sensor1_tensor = data_pd['0'][start_idx:end_idx].values\n",
    "    sensor2_tensor = data_pd['1'][start_idx:end_idx].values\n",
    "    sensor3_tensor = data_pd['2'][start_idx:end_idx].values\n",
    "    sensor4_tensor = data_pd['3'][start_idx:end_idx].values\n",
    "    state_tensor = data_pd['class'][start_idx].values\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "s1=data_pd['1'][0:100]\n",
    "s2=data_pd['0'][0:100]\n",
    "\n",
    "torch.tensor(s1.values, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lilmae/anaconda3/envs/vibration/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dataset import KAMPdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 798/800 [00:00<00:00, 8463.42it/s]\n"
     ]
    }
   ],
   "source": [
    "my_data = KAMPdataset(data_root='./data/test.csv', window_size=256, stride=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[39m=\u001b[39m my_data\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m~/Desktop/BiWavelet_GAN/dataset.py:224\u001b[0m, in \u001b[0;36mKAMPdataset.__getitem__\u001b[0;34m(self, idx, is_biwavelet)\u001b[0m\n\u001b[1;32m    213\u001b[0m     _, _, _, Wcoh, WXdt, freqs, coi \u001b[39m=\u001b[39m xwt(  trace_ref       \u001b[39m=\u001b[39m s1,\n\u001b[1;32m    214\u001b[0m                                             trace_current   \u001b[39m=\u001b[39m s2,\n\u001b[1;32m    215\u001b[0m                                             fs              \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    220\u001b[0m                                             freqmax         \u001b[39m=\u001b[39m \u001b[39m25\u001b[39m,\n\u001b[1;32m    221\u001b[0m                                             nptsfreq        \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(s1))\n\u001b[1;32m    223\u001b[0m     Wcoh_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(Wcoh, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[0;32m--> 224\u001b[0m     x_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39;49mconcatenate(s1,s2),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    225\u001b[0m     y_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n\u001b[1;32m    227\u001b[0m \u001b[39mreturn\u001b[39;00m  torch\u001b[39m.\u001b[39mtensor(np\u001b[39m.\u001b[39mconcatenate(s1,s2),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64), torch\u001b[39m.\u001b[39mtensor(state, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat64)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "sample = my_data.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vibration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
